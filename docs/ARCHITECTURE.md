# ğŸ—ï¸ Architecture & Technical Design

Comprehensive guide to Magic Mirror's system architecture, data flows, and implementation details.

## ğŸ¯ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Browser (Client)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Three.js 3D Renderer                               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚
â”‚  â”‚  â”‚ FBX Avatar Model â”‚â—„â”€â”€â”€â”€â”€â”¤ Blendshape Data  â”‚     â”‚    â”‚
â”‚  â”‚  â”‚ Lynx, Frank,     â”‚      â”‚ (100+ shapes)    â”‚     â”‚    â”‚
â”‚  â”‚  â”‚ Mirror           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚    â”‚
â”‚  â”‚         â”‚                                           â”‚    â”‚
â”‚  â”‚         â”œâ”€ face-api.js (Emotion Detection)          â”‚    â”‚
â”‚  â”‚         â”‚  â”œâ”€ TinyFaceDetector                      â”‚    â”‚
â”‚  â”‚         â”‚  â””â”€ FaceExpressionNet                     â”‚    â”‚
â”‚  â”‚         â”‚                                           â”‚    â”‚
â”‚  â”‚         â”œâ”€ WebAudio API (Microphone Capture)        â”‚    â”‚
â”‚  â”‚         â”‚  â””â”€ MediaRecorder                         â”‚    â”‚
â”‚  â”‚         â”‚                                           â”‚    â”‚
â”‚  â”‚         â””â”€ WebGL Canvas (Video Display)             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                   â”‚
â”‚                         â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  API Communication Layer                            â”‚    â”‚
â”‚  â”‚  â”œâ”€ POST /api/conversation (Main endpoint)          â”‚    â”‚
â”‚  â”‚  â”œâ”€ GET /api/characters (List avatars)              â”‚    â”‚
â”‚  â”‚  â”œâ”€ GET /api/models (List A2F models)               â”‚    â”‚
â”‚  â”‚  â””â”€ POST /api/process-audio (Direct A2F)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTPS/REST
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Deno Server (Backend)                     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Oak HTTP Framework                                 â”‚    â”‚
â”‚  â”‚  â”œâ”€ Route Handler: /talk, /face, /settings          â”‚    â”‚
â”‚  â”‚  â”œâ”€ Middleware: Logger, CORS                        â”‚    â”‚
â”‚  â”‚  â””â”€ Static Files: public/, characters/              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      â”‚                              â”‚    â”‚
â”‚  â–¼                      â–¼                              â–¼    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚ â”‚ OpenAI API   â”‚  â”‚ NVIDIA A2F   â”‚  â”‚ Config       â”‚    â”‚   â”‚
â”‚ â”‚              â”‚  â”‚ gRPC Service â”‚  â”‚ Loader       â”‚    â”‚   â”‚
â”‚ â”‚ â”œâ”€ Whisper   â”‚  â”‚              â”‚  â”‚              â”‚    â”‚   â”‚
â”‚ â”‚ â”‚  (STT)     â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”œâ”€ .env      â”‚    â”‚   â”‚
â”‚ â”‚ â”œâ”€ ChatGPT   â”‚  â”‚ â”‚gRPC Protoâ”‚ â”‚  â”‚ â”‚ Loader     â”‚    â”‚   â”‚
â”‚ â”‚ â”‚  (Chat)    â”‚  â”‚ â”‚ Bindings â”‚ â”‚  â”‚ â”œâ”€ YAML      â”‚    â”‚   â”‚
â”‚ â”‚ â”œâ”€ TTS       â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚ Parser     â”‚    â”‚   â”‚
â”‚ â”‚ â”‚  (Voice)   â”‚  â”‚              â”‚  â”‚ â””â”€ Env       â”‚    â”‚   â”‚
â”‚ â”‚ â””â”€â”€EncProvâ”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   Vars       â”‚    â”‚   â”‚
â”‚ â”‚  (Mammouth.ai   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚ â”‚   optional)     â”‚ Audio       â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ Processor   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚ â”‚                 â”‚             â”‚   â”‚ Blendshape   â”‚    â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ PCM16    â”‚   â”‚ Utils        â”‚    â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ Normalizeâ”‚   â”‚              â”‚    â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ Resample â”‚   â”‚ â”œâ”€ Apply     â”‚    â”‚   â”‚
â”‚ â”‚                 â”‚ â””â”€ Encode   â”‚   â”‚ â”‚ Config     â”‚    â”‚   â”‚
â”‚ â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”œâ”€ Smooth    â”‚    â”‚   â”‚
â”‚ â”‚                                   â”‚ â””â”€ Normalize â”‚    â”‚   â”‚
â”‚ â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚ â”‚                 â”‚ Conversationâ”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ Pipeline    â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚             â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ STT      â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ Chat     â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ TTS      â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ â”œâ”€ A2F      â”‚                       â”‚   â”‚
â”‚ â”‚                 â”‚ â””â”€ Respond  â”‚                       â”‚   â”‚
â”‚ â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”€â”€â”   â”‚
â”‚  â”‚  Data Models                                        â”‚   â”‚
â”‚  â”‚  â”œâ”€ OpenAIConfig (provider, model, voice, key)      â”‚   â”‚
â”‚  â”‚  â”œâ”€ AudioStats (duration, samples, bytes)          â”‚   â”‚
â”‚  â”‚  â”œâ”€ ConversationResult (messages, audio, tokens)   â”‚   â”‚
â”‚  â”‚  â”œâ”€ BlendshapeConfig (emotions, values)            â”‚   â”‚
â”‚  â”‚  â””â”€ ModelConfig (mark_v2_3, claire_v2_3, etc.)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Flow Diagram

### Voice Conversation Flow (Detailed)

```
CLIENT                              SERVER                    EXTERNAL APIs
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 1. User speaks                   â”‚                            â”‚
  â”‚  (Microphone Input)               â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 2. Stop recording                â”‚                            â”‚
  â”‚  (PCM16 @ 16kHz)                  â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 3. Encode to base64 WAV          â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 4. POST /api/conversation â”€â”€â”€â”€â”€â”€â–ºâ”‚                            â”‚
  â”‚  { audio, systemPrompt, emotion } â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 5. Decode base64         â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 6. Send to Whisper â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                   â”‚  (STT)                     â”‚ OpenAI
  â”‚                                   â”‚                            â”‚ Whisper
  â”‚                                   â”‚â—„â”€ 7. Return text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                   â”‚  "What's the weather?"     â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 8. Build system prompt   â”‚
  â”‚                                   â”‚  + emotion context         â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 9. Send to ChatGPT â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                   â”‚  (Chat completion)         â”‚ OpenAI
  â”‚                                   â”‚                            â”‚ ChatGPT
  â”‚                                   â”‚â—„â”€ 10. Return response â”€â”€â”€â”€â”¤
  â”‚                                   â”‚  "I don't have real-time..." â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 11. Send to TTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                                   â”‚  (Text-to-speech)          â”‚ OpenAI
  â”‚                                   â”‚                            â”‚ TTS
  â”‚                                   â”‚â—„â”€ 12. Return MP3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 13. Send to Audio2Faceâ”€â”€â–ºâ”‚
  â”‚                                   â”‚  (Blendshape generation)   â”‚ NVIDIA
  â”‚                                   â”‚                            â”‚ A2F
  â”‚                                   â”‚â—„â”€ 14. Get blendshapes â”€â”€â”€â”€â”¤
  â”‚                                   â”‚  animation_id, values      â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 15. Apply blendshape cfg â”‚
  â”‚                                   â”‚  (smoothing, normalization)â”‚
  â”‚                                   â”‚                            â”‚
  â”‚                                   â”œâ”€ 16. Prepare response     â”‚
  â”‚                                   â”‚  JSON with audio + metadataâ”‚
  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                            â”‚
  â”‚  17. Receive response              â”‚                            â”‚
  â”‚  { userMessage, assistantMessage,  â”‚                            â”‚
  â”‚    audio, tokens, duration }       â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 18. Decode MP3 audio             â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 19. Play audio via Web Audio API â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 20. Update blendshapes frame    â”‚                            â”‚
  â”‚  by frame as audio plays          â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 21. Three.js renders avatar     â”‚                            â”‚
  â”‚  with animated mouth sync         â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â”œâ”€ 22. Display response text        â”‚                            â”‚
  â”‚  "I don't have real-time..."      â”‚                            â”‚
  â”‚                                   â”‚                            â”‚
  â””â”€ Ready for next input             â”‚                            â”‚
```

## ğŸ”„ Conversation Pipeline (Sequential Calls)

```typescript
async function conversationPipeline(
  audioBase64: string,
  systemPrompt: string,
  emotion?: string
): Promise<ConversationResult> {
  // 1. STT: Audio â†’ Text
  const userMessage = await transcribeAudio(audioBase64);

  // 2. Chat: User Text â†’ AI Response
  const assistantMessage = await generateChatResponse(
    userMessage,
    systemPrompt,
    emotion
  );

  // 3. TTS: Response Text â†’ Audio (MP3)
  const audioBuffer = await generateSpeech(assistantMessage);
  const audioBase64 = bufferToBase64(audioBuffer);

  // 4. A2F: Audio â†’ Blendshape Animation Data
  const blendshapes = await processAudioWithA2F(audioBase64);

  // 5. Return combined result
  return {
    userMessage,
    assistantMessage,
    audio: audioBase64,
    blendshapes,
    tokens: { prompt, completion, total },
    audioStats: { duration, sampleCount, byteSize },
  };
}
```

## ğŸ¬ 3D Avatar Rendering

### FBX Model Loading

```typescript
// Browser-side (Three.js)
const fbxLoader = new FBXLoader();
fbxLoader.load("/characters/lynq/lynx_bobcat_01.fbx", (fbx) => {
  // fbx = THREE.Group with:
  //  â”œâ”€ mesh (geometry + materials)
  //  â”œâ”€ skeleton (bone structure)
  //  â”œâ”€ animations (idle, etc)
  //  â””â”€ blendshapes (100+ facial shapes)

  scene.add(fbx);
});
```

### Blendshape Application

```typescript
// Update blendshape weights per frame
function updateBlendshapes(blendshapeData) {
  for (const [shapeName, value] of Object.entries(blendshapeData)) {
    mesh.morphTargetInfluences[shapeIndex] = value; // 0-1
  }
}

// Sync with audio playback
audioElement.addEventListener("timeupdate", () => {
  const frameIndex = Math.floor(currentTime / frameDelta);
  updateBlendshapes(blendshapes[frameIndex]);
});
```

## ğŸ”Š Audio Processing Pipeline

### Input: WebAudio API Microphone

```
Raw Microphone Input (AudioBuffer)
  â†“ (Sample rate may vary: 44.1kHz, 48kHz, 96kHz)
Convert to 16-bit PCM @ 16kHz
  â†“ (Whisper requirement)
Base64 encode as WAV
  â†“
Send to server
```

### Processing: Audio Normalization

```typescript
function normalizeAudio(audioBuffer: AudioBuffer): Float32Array {
  // 1. Get single channel (mono)
  const rawData = audioBuffer.getChannelData(0);

  // 2. Find max amplitude
  const max = Math.max(...Array.from(rawData));

  // 3. Normalize to -1.0 to 1.0 range
  const normalized = rawData.map((sample) => sample / max);

  // 4. Convert to Int16 for PCM16
  const int16 = new Int16Array(normalized.length);
  for (let i = 0; i < normalized.length; i++) {
    int16[i] = normalized[i] * 32767; // Max 16-bit signed
  }

  return int16;
}
```

### Output: TTS MP3 to Blendshape Timeline

```
OpenAI TTS Output (MP3)
  â†“ (Encoded audio, duration ~2-5 seconds)
Decode MP3 in browser
  â†“ (AudioBuffer or typed array)
Resample to 16kHz if needed
  â†“
Send to Audio2Face (NVIDIA gRPC)
  â†“
Receives blendshape keyframes:
  {
    animation_id: "...",
    blendshapes: [
      { frame: 0, values: [0.1, 0.2, 0.3, ...] },
      { frame: 1, values: [0.12, 0.22, 0.35, ...] },
      ...
    ]
  }
  â†“
Apply to avatar blendshape targets
  â†“
Sync playback with audio timeline
```

## ğŸ“ Module Organization

### `src/openai.ts` - AI Provider Abstraction

```
OpenAIConfig interface
  â”œâ”€ apiKey: string
  â”œâ”€ model: string
  â”œâ”€ voice: string
  â”œâ”€ provider: "openai" | "mammouth"
  â”œâ”€ baseUrl: string (optional)
  â””â”€ maxTokens: number

Functions:
  â”œâ”€ getApiBaseUrl() â†’ Dynamic endpoint
  â”œâ”€ transcribeAudio() â†’ STT (Whisper only)
  â”œâ”€ generateChatResponse() â†’ Chat (OpenAI or Mammouth)
  â”œâ”€ generateSpeech() â†’ TTS (OpenAI only)
  â””â”€ conversationPipeline() â†’ Full flow (STTâ†’Chatâ†’TTSâ†’A2F)
```

### `src/nvidia/` - Audio2Face Module

```
â”œâ”€ index.ts (Public exports)
â”œâ”€ constants.ts (PCM16_SAMPLE_RATE, CHUNK_DURATION)
â”œâ”€ models.ts (Mark v2.3, Claire v2.3, James v2.3)
â”œâ”€ audio-processor.ts (Normalize, resample, encode)
â”œâ”€ config-loader.ts (Load YAML, parse emotions)
â””â”€ service.ts (Main processAudioWithA2F() function)

Types:
  â”œâ”€ AudioStats { duration, sampleCount, byteSize }
  â”œâ”€ BlendshapeFrame { frame, values }
  â”œâ”€ A2FResponse { animation_id, blendshapes }
  â””â”€ ModelConfig { blendshapes, emotions }
```

### `public/emotion-detector.js` - Webcam ML

```
EmotionDetector class
  â”œâ”€ initialize() â†’ Request camera + load models
  â”œâ”€ startDetection() â†’ Continuous inference
  â”œâ”€ detectEmotion() â†’ Single frame analysis
  â”œâ”€ getEmotion() â†’ Current emotion
  â”œâ”€ getAverageEmotion() â†’ Smoothed emotion
  â”œâ”€ getPersonalizedGreeting() â†’ Response text
  â””â”€ onEmotionUpdate(callback) â†’ Real-time updates

Models:
  â”œâ”€ TinyFaceDetector (face detection box)
  â””â”€ FaceExpressionNet (7 emotions)
```

### `public/voice-conversation.js` - Microphone & API

```
VoiceConversation class
  â”œâ”€ initialize() â†’ Request microphone
  â”œâ”€ startRecording() / stopRecording()
  â”œâ”€ sendConversation() â†’ POST /api/conversation
  â”œâ”€ playResponse() â†’ Web Audio API playback
  â”œâ”€ conversationLoop() â†’ Full cycle
  â”œâ”€ setEmotionDetector() â†’ Attach ML model
  â””â”€ dispose() â†’ Cleanup

Data:
  â”œâ”€ mediaRecorder (Audio capture)
  â”œâ”€ audioChunks (WAV data)
  â””â”€ audioContext (Web Audio API)
```

## ğŸ” Environment Configuration

### .env Resolution Order

```
1. Check .env file in project root
2. Parse with loadEnv() from src/config.ts
3. Merge into Deno.env
4. Fall back to process.env if not in Deno.env
5. Use defaults if all missing
```

### Configuration Flow

```
.env file
  â†“
loadEnv() parser
  â†“
Deno.env.set()
  â†“
OpenAI module checks:
  getRequiredConfig("OPENAI_API_KEY")
  getConfig("MAMMOUTH_API_KEY", "")
  getConfig("OPENAI_MODEL", "gpt-4o-mini")
  â†“
Used in API calls
```

## ğŸ¯ Performance Considerations

### Latency Breakdown (Typical)

```
User speaks          â†’ Audio capture:        1-5 seconds
Stop â†’ Send          â†’ Network:              0.1-0.5 seconds
Network â†’ Server     â†’ Processing:           0.1 seconds
Whisper (STT)        â†’ Server:               1-3 seconds
ChatGPT (Chat)       â†’ Server:               1-3 seconds
TTS (Speech)         â†’ Server:               0.5-1 seconds
Audio2Face (A2F)     â†’ Server:               1-2 seconds
Network â†’ Browser    â†’ Network:              0.1-0.5 seconds
Playback start       â†’ Browser:              0.1 seconds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total                                        ~5-15 seconds
```

### Memory Usage

```
Browser (Client-side):
  â”œâ”€ Three.js scene:         ~30-50 MB (FBX + textures)
  â”œâ”€ face-api.js models:     ~100 KB (cached)
  â”œâ”€ Audio buffers:          ~5-10 MB (recording)
  â””â”€ Canvas + rendering:     ~10-20 MB
  Total:                      ~50-80 MB

Server (Deno runtime):
  â”œâ”€ Oak framework:           ~20 MB
  â”œâ”€ OpenAI SDK:              ~10 MB
  â”œâ”€ gRPC connections:        ~5 MB
  â””â”€ Module cache:            ~10 MB
  Total:                       ~50-100 MB
```

### Network Bandwidth

```
Per conversation:
  â”œâ”€ Upload (WAV audio):      ~200-500 KB
  â”œâ”€ Download (MP3 audio):    ~50-200 KB
  â”œâ”€ JSON metadata:           ~5-10 KB
  â””â”€ Blendshape data:         ~20-50 KB
  Total:                       ~300-800 KB
```

## ğŸ”Œ API Contract

### POST `/api/conversation`

**Request Schema:**

```typescript
interface ConversationRequest {
  audio: string; // base64 encoded WAV
  systemPrompt?: string; // Character personality prompt
  character?: string; // Avatar model (mark_v2_3, etc)
  emotion?: string; // Detected emotion context
}
```

**Response Schema:**

```typescript
interface ConversationResponse {
  userMessage: string; // Transcribed user input
  assistantMessage: string; // AI generated response
  audio: string; // base64 encoded MP3
  audioStats: {
    duration: number; // seconds
    sampleCount: number; // PCM samples
    byteSize: number; // bytes
  };
  tokens: {
    prompt: number; // Input tokens
    completion: number; // Output tokens
    total: number; // Sum
  };
}
```

## ğŸ§ª Testing Strategy

### Unit Tests

```typescript
// Test audio normalization
const input = new Float32Array([0.5, 1.0, 0.8]);
const output = normalizeAudio(input);
assert(output[0] >= -1.0 && output[0] <= 1.0);

// Test emotion detection
const emotions = await detectEmotion(faceData);
assert(emotions.happy + emotions.sad + ... === 1.0);
```

### Integration Tests

```typescript
// Test full conversation
const response = await conversationPipeline(audioBase64, "Be helpful", "happy");
assert(response.userMessage.length > 0);
assert(response.assistantMessage.length > 0);
assert(response.audio.length > 0);
```

### Performance Tests

```typescript
// Benchmark API call times
const start = performance.now();
const result = await generateChatResponse("Hello");
const duration = performance.now() - start;
console.log(`Chat response: ${duration}ms`);
// Target: < 3 seconds for gpt-4o-mini
```

## ğŸ“š References

- **Deno**: https://deno.com
- **Oak**: https://oak.deno.dev
- **Three.js**: https://threejs.org
- **face-api.js**: https://github.com/justadudewhohacks/face-api.js
- **NVIDIA Audio2Face**: https://developer.nvidia.com/ace
- **OpenAI API**: https://platform.openai.com/docs
